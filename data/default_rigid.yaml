# Whether to use bias correction before registration (helpful to generate more accurate transforms at the cost of time). Default: false
bias: false

# The registration metric. Options: ["mattes_mutual_information", "ants_neighborhood_correlation", "correlation", "demons", "joint_histogram_mutual_information", "mean_squares"]
# this can be a list as well, which will then become a multi-metric registration using composite transforms [https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1CompositeTransform.html]
metric: mean_squares

## metric-specific parameters
metric_parameters: {
  histogram_bins: 50, # mattes_mutual_information, joint_histogram_mutual_information
  radius: 5, # ants_neighborhood_correlation
  intensityDifferenceThreshold: 0.001, # demons
  varianceForJointPDFSmoothing: 1.5, # joint_histogram_mutual_information
}

# Optimizer. Options: ["gradient_descent", "regular_step_gradient_descent", "amoeba", "conjugate", "exhaustive", "gadient_descent_line_search", "lbfgsb", "lbfgsb2", "one_plus_one_evolutionary", "powell"]
optimizer: regular_step_gradient_descent

## optimizer-specific parameters
optimizer_parameters: {
  min_step: 1e-4, # regular_step_gradient_descent
  max_step: 1.0, # gradient_descent, regular_step_gradient_descent
  iterations: 200, # regular_step_gradient_descent, gradient_descent_line_search, gradient_descent, conjugate, lbfgsb, lbfgsb2
  relaxation: 0.1, # regular_step_gradient_descent
  learningrate: 1.0, # gradient_descent, gradient_descent_line_search
  tolerance: 1e-6, # gradient_descent, gradient_descent_line_search
  convergence_minimum: 1e-6, # gradient_descent, gradient_descent_line_search
  convergence_window_size: 10, # gradient_descent, gradient_descent_line_search
  line_search_lower_limit: 0.0, # gradient_descent_line_search
  line_search_upper_limit: 5.0, # gradient_descent_line_search
  line_search_epsilon: 0.01, # gradient_descent_line_search
  step_length: 0.1, # conjugate, exhaustive, powell
  simplex_delta: 0.1, # amoeba
  maximum_number_of_corrections: 5, # lbfgsb, lbfgsb2
  maximum_number_of_function_evaluations: 2000, # lbfgsb, lbfgsb2
  solution_accuracy: 1e-5, # lbfgsb2
  hessian_approximate_accuracy: 1e-5, # lbfgsb2
  delta_convergence_distance: 1e-5, # lbfgsb2
  delta_convergence_tolerance: 1e-5, # lbfgsb2
  line_search_maximum_evaluations: 50, # lbfgsb2
  line_search_minimum_step: 1e-20, # lbfgsb2
  line_search_accuracy: 1e-4, # lbfgsb2
  epsilon: 1e-8, # one_plus_one_evolutionary
  initial_radius: 1.0, # one_plus_one_evolutionary
  growth_factor: -1.0, # one_plus_one_evolutionary
  shrink_factor: -1.0, # one_plus_one_evolutionary
  maximum_line_iterations: 100, # powell
  step_tolerance: 1e-6, # powell
  value_tolerance: 1e-6, # powell
}

# The registration transform. Options: ["translation", "versor", "versor_rigid", "euler", "similarity", "scale", "scale_versor", "scale_skew_versor", "affine", "bspline", "displacement"]
transform: versorrigid

# Composite transform
composite_transform: false

# Previous transforms saved to disk: only used if composite_transform is true
previous_transforms: []

# Transform initialization. Options: ["moments", "geometry", "selfmoments", "selfgeometry"]
initialization: geometry

# Interpolator. Options: ["linear", "bspline", "nearestneighbor", "gaussian", "labelgaussian"]
interpolator: linear

# Sampling strategy. Options: ["regular", "random", "none"]
sampling_strategy: "random"

# Sampling percentage. Can be a list of percentages with the same length as the number of levels.
sampling_percentage: 0.01

# Shrink factor at each level for pyramid registration.
shrink_factors: [4, 2, 1]

# Smoothing sigma at each level for pyramid registration.
smoothing_sigmas: [2, 1, 0]

# The number of attempts to try to find a good registration (useful when using random sampling)
attempts: 5
